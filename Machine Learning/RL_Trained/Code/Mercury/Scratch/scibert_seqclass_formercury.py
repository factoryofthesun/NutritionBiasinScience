# -*- coding: utf-8 -*-
"""SciBERT_for_foodsci_sentiment_BertforSeqClass.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14WZYL03zadZc4DwPGEoLhgFFxXjfEeQi

Code adapted from [this tutorial](https://mccormickml.com/2019/07/22/BERT-fine-tuning/#11-using-colab-gpu-for-training)

SciBERT repo [here](https://github.com/allenai/scibert)
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x
from sklearn.model_selection import train_test_split
import tensorflow as tf
import torch
import pandas as pd
import numpy as np
import time
import datetime
import random

#Import tensorflow and torch and make sure we're on a GPU
# If there's a GPU available...
if torch.cuda.is_available():

    # Tell PyTorch to use the GPU.
    device = torch.device("cuda")

    print('There are %d GPU(s) available.' % torch.cuda.device_count())

    print('We will use the GPU:', torch.cuda.get_device_name(0))

# If not...
else:
    print('No GPU available, using the CPU instead.')
    device = torch.device("cpu")

#Manually upload the data
'''from google.colab import files

uploaded = files.upload()

for fn in uploaded.keys():
  print('User uploaded file "{name}" with length {length} bytes'.format(
      name=fn, length=len(uploaded[fn])))'''

#Load data and apply polarity rule
from pathlib import Path

data_path = str(Path(__file__).parent / "../Data")
mturk_abstracts = pd.read_csv(data_path + "/mturk_train.csv")

#Polarity rule: If >=2 positive ratings, then label positive
mturk_abstracts['polarity'] = (mturk_abstracts['count_pos'] >= 2).astype(int)

abstracts = mturk_abstracts['inputtext'].tolist()
labels = mturk_abstracts['polarity'].tolist()

assert len(abstracts) == len(labels)

#Check for NAN values
print(len(mturk_abstracts.loc[(mturk_abstracts.inputtext == "")| (mturk_abstracts.inputtext.isnull()),]))
print(len(mturk_abstracts.loc[(mturk_abstracts.polarity == "") | (mturk_abstracts.polarity.isnull()),]))
print(set(labels))

#Load SciBERT tokenizer
from transformers import AutoTokenizer

print("Loading SciBERT tokenizer...")
tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')

#Define method to truncate from center
def truncateCenter(s, maxlen):
    ret = s
    while len(ret) > maxlen:
        ret = ret[:len(ret)//2-1] + ret[len(ret)//2+1:] #Remove middle two tokens
    return ret

#Encode abstracts, set truncation and padding, and return as PyTorch tensors
MAX_LEN = 512 #Longest BERT input length

input_tensors_list = []
attention_tensors_list = []

for a in abstracts:
  tokenized = tokenizer.tokenize(a) #Tokenize first
  trunc_tokens = truncateCenter(tokenized, MAX_LEN-2) #Truncate from center, leave room for tokens
  if len(trunc_tokens) > 510:
      print("Truncation error - found token list with length {}".format(len(trunc_tokens)))
  encoded_dict = tokenizer.encode_plus(trunc_tokens, max_length = MAX_LEN, pad_to_max_length=True,
                                  add_special_tokens=True, return_attention_mask = True,
                                  return_tensors='pt', truncation_strategy = 'do_not_truncate')
  input_tensors_list.append(encoded_dict['input_ids'])
  attention_tensors_list.append(encoded_dict['attention_mask'])

#Convert lists into tensors
input_tensor = torch.cat(input_tensors_list, dim=0)
attention_mask_tensor = torch.cat(attention_tensors_list, dim=0)
label_tensor = torch.tensor(labels)

#Check encoded values
print("Input tensor size:", input_tensor.size())

#Create tensor datasets then split into training/validation
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, random_split

def create_dataloaders(bsize, input_tensor, attention_mask_tensor, label_tensor):
  batch_size = bsize #Fit as many as possible in memory

  #Split 90-10 train-test
  train_num = int(len(abstracts) * 0.9)
  test_num = len(abstracts) - train_num

  #Create training and testing sets
  input_tdata = TensorDataset(input_tensor, attention_mask_tensor, label_tensor)
  train_data, test_data = random_split(input_tdata, [train_num, test_num])

  #Create DataLoaders
  train_sampler = RandomSampler(train_data)
  train_dataloader = DataLoader(train_data, sampler = train_sampler, batch_size = batch_size)

  test_sampler = SequentialSampler(test_data)
  test_dataloader = DataLoader(test_data, sampler = test_sampler, batch_size = batch_size)

  return train_dataloader, test_dataloader

#Confirm positive counts

'''train_pos_count = sum([int(torch.eq(x[2], 1)) for x in train_data])
test_pos_count = sum([int(torch.eq(x[2], 1)) for x in test_data])
print("Training data has {} positive polarity out of {} samples".format(train_pos_count, len(train_data)))
print("Testing data has {} positive polarity out of {} samples".format(test_pos_count, len(test_data)))'''

#Load SciBERT sequence classification model

#Define helper functions
#Accuracy function
def flat_accuracy(preds, labels):
    pred_flat = np.argmax(preds, axis=1).flatten()
    labels_flat = labels.flatten()
    return np.sum(pred_flat == labels_flat) / len(labels_flat)

def format_time(elapsed):
    '''
    Takes a time in seconds and returns a string hh:mm:ss
    '''
    # Round to the nearest second.
    elapsed_rounded = int(round((elapsed)))

    # Format as hh:mm:ss
    return str(datetime.timedelta(seconds=elapsed_rounded))

#Training code
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
path = str(Path(__file__).parent / "../Plots")

def train(model, optimizer, scheduler, train_dataloader, test_dataloader, epochs):
  #Set seed
  seed_val = 42
  random.seed(seed_val)
  np.random.seed(seed_val)
  torch.manual_seed(seed_val)
  torch.cuda.manual_seed_all(seed_val)

  #Record loss and accuracy
  train_loss = []
  test_loss = []
  train_acc = []
  test_acc = []
  t_start = time.time()

  for epoch_i in range(epochs):
    #====================
    #     Training
    #====================
    print("")
    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))
    print('Training...')
    t0 = time.time()
    total_loss = 0
    total_accuracy = 0
    steps = 0
    model.train() #Put into training mode

    for step, batch in enumerate(train_dataloader):
      #Progress update every 5 batches
      if step % 5 == 0 and not step == 0:
        elapsed = format_time(time.time() - t0)
        print('  Batch {}  of  {}.    Elapsed: {}.'.format(step, len(train_dataloader), elapsed))

      #Unpack batch and copy tensors to GPU
      b_input_ids = batch[0].to(device)
      b_input_mask = batch[1].to(device)
      b_labels = batch[2].to(device)

      #Make sure to zero out the gradient
      model.zero_grad()

      #Perform forward pass
      outputs = model(b_input_ids,
                      token_type_ids=None,
                      attention_mask=b_input_mask,
                      labels=b_labels)

      #Get loss
      loss = outputs[0]
      total_loss += loss.item()
      logits = outputs[1] #Logits are the output values prior to applying activation function

      #Move logits and labels to CPU - can't perform accuracy calculation on GPU
      logits = logits.detach().cpu().numpy()
      label_ids = b_labels.to('cpu').numpy()

      #Calculate batch accuracy
      tmp_accuracy = flat_accuracy(logits, label_ids)
      total_accuracy += tmp_accuracy
      steps += 1

      #Backward pass to calculate gradients
      loss.backward()

      #Clip norms to prevent "exploding gradients"
      torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

      optimizer.step() #Update parameters using optimizer
      scheduler.step() #Update learning rate

    #Average loss/accuracy
    avg_loss = total_loss/steps
    avg_acc = total_accuracy/steps

    train_loss.append(avg_loss)
    train_acc.append(avg_acc)
    print("")
    print("  Average training loss: {0:.2f}".format(avg_loss))
    print("  Average training accuracy: {0:.2f}".format(avg_acc))
    print("  Training epoch took: {:}".format(format_time(time.time() - t0)))

    #====================
    #     Validation
    #====================
    print("")
    print("Running validation...")
    t0 = time.time()
    total_loss = 0
    total_accuracy = 0
    steps = 0

    model.eval() #Evaluation mode

    for batch in test_dataloader:
      #Unpack batch and copy tensors to GPU
      b_input_ids = batch[0].to(device)
      b_input_mask = batch[1].to(device)
      b_labels = batch[2].to(device)

      #Tell model not to store gradients - speeds up testing
      with torch.no_grad():
        outputs = model(b_input_ids,
                        token_type_ids=None,
                        attention_mask=b_input_mask,
                        labels = b_labels)

      #Get loss
      loss = outputs[0]
      total_loss += loss.item()
      logits = outputs[1] #Logits are the output values prior to applying activation function

      #Move logits and labels to CPU - can't perform accuracy calculation on GPU
      logits = logits.detach().cpu().numpy()
      label_ids = b_labels.to('cpu').numpy()

      #Calculate batch accuracy
      tmp_accuracy = flat_accuracy(logits, label_ids)
      total_accuracy += tmp_accuracy

      steps += 1 #Track batch num
    #Report the final accuracy for this validation run.
    test_loss.append(total_loss/steps)
    test_acc.append(total_accuracy/steps)
    print("  Average validation loss: {0:.2f}".format(total_loss/steps))
    print("  Average validation accuracy: {0:.2f}".format(total_accuracy/steps))
    print("  Validation took: {:}".format(format_time(time.time() - t0)))

  print("")
  print("Training complete!")
  print("Total training took {:} (h:mm:ss)".format(format_time(time.time()-t_start)))

  #Plot results
  loss_fname = "loss_lr{}_hiddendp{}_attentiondp{}.png".format(lr, hidden_dp_prob, attention_dp_prob)
  acc_fname = "accuracy_lr{}_hiddendp{}_attentiondp{}.png".format(lr, hidden_dp_prob, attention_dp_prob)

  plt.plot(train_loss, 'r--')
  plt.plot(test_loss, 'b-')
  plt.legend(['Training Loss', 'Validation Loss'])
  plt.title("Loss by Epoch")
  plt.xlabel("Epoch")
  plt.ylabel("Loss")
  plt.savefig(path + '/' + loss_fname)
  plt.clf()

  plt.plot(train_acc, 'r--')
  plt.plot(test_acc, 'b-')
  plt.legend(['Training Accuracy', 'Validation Accuracy'])
  plt.title("Accuracy by Epoch")
  plt.xlabel("Epoch")
  plt.ylabel("Accuracy")
  plt.savefig(path + '/' + acc_fname)
  plt.clf()

from transformers import BertForSequenceClassification, AdamW, BertConfig
from transformers import get_linear_schedule_with_warmup

#Train on different hyperparameters
for lr in [2e-5, 3e-5, 5e-5]:
  for hidden_dp_prob in [.1, .5, .9]:
    for attention_dp_prob in [.1, .5, .9]:

      train_dataloader, test_dataloader = create_dataloaders(10, input_tensor, attention_mask_tensor, label_tensor)

      #Create model
      model = BertForSequenceClassification.from_pretrained("allenai/scibert_scivocab_uncased",
                                                      num_labels = 2,
                                                      output_attentions = False,
                                                      output_hidden_states = False,
                                                      hidden_dropout_prob=hidden_dp_prob,
                                                      attention_probs_dropout_prob=attention_dp_prob)
      model.cuda()

      #Set optimizer
      optimizer = AdamW(model.parameters(),
                        lr = lr,
                        eps = 1e-8)

      epochs = 10
      total_steps = len(train_dataloader) * epochs

      #Create learning rate scheduler
      scheduler = get_linear_schedule_with_warmup(optimizer,
                                                  num_warmup_steps = 0,
                                                  num_training_steps = total_steps)

      train(model, optimizer, scheduler, train_dataloader, test_dataloader, epochs)
