# Save version of wos file with cleaned publiation year and keywords
nrow(wos_data[is.na(DE)])
# Save version of wos file with cleaned publiation year and keywords
nrow(wos_data[DE == ""])
wos_data[is.na(DE) | DE == "", DE := ID]
nrow(wos_data[DE == "" | is.na(DE)])
nrow(wos_data[DE == "" | ID == ""])
nrow(wos_data[DE == "" & ID == ""])
write.csv(wos_data, "../WebOfScience/Industry_Tagging/Outputs/wos_indtagged_final_wide_FEclean.csv", row.names=F)
#==============================================================================
# Script to take the results-sentiment classified abstracts and perform some
# initial analysis.
#==============================================================================
rm(list=ls())
library(ggplot2)
library(stargazer)
library(data.table)
library(dplyr)
# Read in data
setwd("/Volumes/GoogleDrive/.shortcut-targets-by-id/0B0efpUDNVRiVME9hOFNHaU1xVW8/Nutrition_BiasInScience/Regression_Analysis")
full_predictions <- read.csv("../Machine Learning/RL_Trained/Predictions/full_predictions_AdamW_grains.csv")
pred_dt <- setDT(full_predictions)
pred_dt <- na.omit(pred_dt) # Remove faulty na data
pred_dt <- pred_dt[,c("Abstract.Code", "Prob_NotPositive", "Prob_Positive", "Prediction")]
wos_indtagged <- read.csv("../WebOfScience/Industry_Tagging/Outputs/wos_indtagged_final_wide_FEclean.csv")
wos_indtagged <- setDT(wos_indtagged)
# Summary statistics data
summ_stats <- setDT(read.csv("../WebOfScience/Industry_Tagging/Outputs/Food Category Summary Statistics_v5.csv"))
summ_stats_nomiss <- rbind(summ_stats[!(Food.Code %in% c(134,166))],summ_stats[!(Food.Code %in% c(134,166))])
summ_stats_withmiss <- rbind(summ_stats, summ_stats)
# Create industry label variables
wos_indtagged[Is_Industry==0, Industry_Labels := "Not Industry Funded"]
wos_indtagged[Is_Industry==1, Industry_Labels := "Industry Funded"]
wos_indtagged[FU=="", Industry_Labels := "Missing Funding Data"]
# Top 100 industry labels
wos_indtagged[Is_Industry_Top100==0, Top100_Industry_Labels := "Not Top100 Industry Funded"]
wos_indtagged[Is_Industry_Top100==1, Top100_Industry_Labels := "Top100 Industry Funded"]
wos_indtagged[FU=="", Top100_Industry_Labels := "Missing Funding Data"]
industry_for_reg <- wos_indtagged[,c("Abstract.Code","Food.Code", "Food.Name","Is_Industry", "Is_Industry_Top100",
"Industry_Labels", "Top100_Industry_Labels", "SO"), with=FALSE]
industry_for_reg <- na.omit(industry_for_reg) # Remove faulty na data
colnames(industry_for_reg)[8] <- "Publication_Name"
# Create measure of "industry concentration" = # top 100 companies/# total companies
industry_counts <- industry_for_reg[,.(Tot_Industry = sum((Is_Industry)), Tot_Top100 = sum((Is_Industry_Top100))), by = Food.Code]
industry_counts$Industry_Concentration <- ifelse(industry_counts$Tot_Industry==0, 0, industry_counts$Tot_Top100/industry_counts$Tot_Industry)
all_industry_concentration <- sum(industry_counts$Tot_Top100)/sum(industry_counts$Tot_Industry)
all_industry_concentration # Full top100 concentration
# Get final data for regression
full_reg_df <- merge(industry_for_reg, pred_dt, by="Abstract.Code") # Output should be same length as pred_dt
full_reg_df$Prediction <- as.factor(full_reg_df$Prediction)
full_reg_df$Food.Code <- as.factor(full_reg_df$Food.Code)
full_reg_df$Is_Industry <- as.factor(full_reg_df$Is_Industry)
full_reg_df$Is_Industry_Top100 <- as.factor(full_reg_df$Is_Industry_Top100)
industry_sentiment_summary <- full_reg_df[,.(Counts=log(.N)),by=list(Industry_Labels, Prediction)]
industry_sentiment_top100_summary <- full_reg_df[,.(Counts=log(.N)),by=list(Top100_Industry_Labels, Prediction)]
# Save a separate regression data file that includes the article title + first 20 words
# The raw text is included so it can be matched on the raw data files in the future
industry_with_text <- wos_indtagged[,c("Abstract.Code","Food.Code", "AB", "TI","Food.Name","Is_Industry", "Is_Industry_Top100",
"Industry_Labels", "Top100_Industry_Labels", "SO", "Pub_Year", "DE"), with=FALSE]
colnames(industry_with_text)[c(4, 10, 12)] <- c("Title", "Journal", "Keywords")
industry_with_text$AB <- substr(industry_with_text$AB, 1,200) # First 200 characters of AB
reg_data <- merge(industry_with_text, pred_dt, by="Abstract.Code", all.y = TRUE)
write.csv(reg_data, "regression_data.csv", row.names=FALSE)
#==============================================================================
# Author: Richard Liu
# Description: Script to compute and visualize the distribution of publication dates between
# missing/non-missing funding sources and between industry/non-industry sources
#==============================================================================
rm(list = ls())
library(dplyr)
library(tidyr)
library(data.table)
library(stringr)
library(gridExtra)
library(ggplot2)
library(chisq.posthoc.test)
setwd("/Volumes/GoogleDrive/.shortcut-targets-by-id/0B0efpUDNVRiVME9hOFNHaU1xVW8/Nutrition_BiasInScience/Regression_Analysis")
wos_data <- setDT(read.csv("../WebOfScience/Industry_Tagging/Outputs/wos_indtagged_final_wide_v1.csv"))
wos_data$Funding_Missing <- (wos_data$FU_stripped_lower == ' missing ')
wos_data$Pub_Year <- str_extract(wos_data$PY, "\\d+") # Extract numeric values from publication year column
wos_data$PD <- str_extract(wos_data$PD, "\\d+")
wos_data$EA <- str_extract(wos_data$EA, "\\d+")
wos_data$Pub_Year <- as.numeric(wos_data$Pub_Year)
wos_data$PD <- as.numeric(wos_data$PD)
wos_data$EA <- as.numeric(wos_data$EA)
wos_data[is.na(PD), PD := 0]
wos_data[is.na(EA), EA := 0]
wos_data[is.na(Pub_Year) | (Pub_Year > 0 & Pub_Year < 1000), Pub_Year := pmax(PD, EA, na.rm = TRUE)]
# Save version of wos file with cleaned publiation year and keywords
wos_data[is.na(DE) | DE == "", DE := ID]
wos_data$DE <- trimws(tolower(gsub('[^;-a-z0-9A-Z\s]', '', wos_data$DE)))
wos_data$DE <- trimws(tolower(gsub('[^;-a-z0-9A-Z\\s]', '', wos_data$DE)))
wos_data$DE <- trimws((gsub('[^;-a-z0-9\\s]', '', tolower(wos_data$DE))))
wos_data$DE <- trimws((gsub('[^-;a-z0-9\\s]', '', tolower(wos_data$DE))))
View(wos_data)
#==============================================================================
# Author: Richard Liu
# Description: Script to compute and visualize the distribution of publication dates between
# missing/non-missing funding sources and between industry/non-industry sources
#==============================================================================
rm(list = ls())
library(dplyr)
library(tidyr)
library(data.table)
library(stringr)
library(gridExtra)
library(ggplot2)
library(chisq.posthoc.test)
setwd("/Volumes/GoogleDrive/.shortcut-targets-by-id/0B0efpUDNVRiVME9hOFNHaU1xVW8/Nutrition_BiasInScience/Regression_Analysis")
wos_data <- setDT(read.csv("../WebOfScience/Industry_Tagging/Outputs/wos_indtagged_final_wide_v1.csv"))
wos_data$Funding_Missing <- (wos_data$FU_stripped_lower == ' missing ')
wos_data$Pub_Year <- str_extract(wos_data$PY, "\\d+") # Extract numeric values from publication year column
wos_data$PD <- str_extract(wos_data$PD, "\\d+")
wos_data$EA <- str_extract(wos_data$EA, "\\d+")
wos_data$Pub_Year <- as.numeric(wos_data$Pub_Year)
wos_data$PD <- as.numeric(wos_data$PD)
wos_data$EA <- as.numeric(wos_data$EA)
wos_data[is.na(PD), PD := 0]
wos_data[is.na(EA), EA := 0]
wos_data[is.na(Pub_Year) | (Pub_Year > 0 & Pub_Year < 1000), Pub_Year := pmax(PD, EA, na.rm = TRUE)]
# Save version of wos file with cleaned publiation year and keywords
wos_data[is.na(DE) | DE == "", DE := ID]
wos_data$DE <- trimws((gsub('[^-\\s;a-z0-9]', '', tolower(wos_data$DE))))
View(wos_data)
#==============================================================================
# Author: Richard Liu
# Description: Script to compute and visualize the distribution of publication dates between
# missing/non-missing funding sources and between industry/non-industry sources
#==============================================================================
rm(list = ls())
library(dplyr)
library(tidyr)
library(data.table)
library(stringr)
library(gridExtra)
library(ggplot2)
library(chisq.posthoc.test)
setwd("/Volumes/GoogleDrive/.shortcut-targets-by-id/0B0efpUDNVRiVME9hOFNHaU1xVW8/Nutrition_BiasInScience/Regression_Analysis")
wos_data <- setDT(read.csv("../WebOfScience/Industry_Tagging/Outputs/wos_indtagged_final_wide_v1.csv"))
wos_data$Funding_Missing <- (wos_data$FU_stripped_lower == ' missing ')
wos_data$Pub_Year <- str_extract(wos_data$PY, "\\d+") # Extract numeric values from publication year column
wos_data$PD <- str_extract(wos_data$PD, "\\d+")
wos_data$EA <- str_extract(wos_data$EA, "\\d+")
wos_data$Pub_Year <- as.numeric(wos_data$Pub_Year)
wos_data$PD <- as.numeric(wos_data$PD)
wos_data$EA <- as.numeric(wos_data$EA)
wos_data[is.na(PD), PD := 0]
wos_data[is.na(EA), EA := 0]
wos_data[is.na(Pub_Year) | (Pub_Year > 0 & Pub_Year < 1000), Pub_Year := pmax(PD, EA, na.rm = TRUE)]
# Save version of wos file with cleaned publiation year and keywords
wos_data[is.na(DE) | DE == "", DE := ID]
View(wos_data)
wos_data$Keywords <- trimws((gsub('[\\S^-;a-z0-9]', '', tolower(wos_data$DE))))
wos_data$Keywords <- trimws((gsub('[^-;a-z0-9\\s]', '', tolower(wos_data$DE))))
View(wos_data)
wos_data$Keywords <- gsub('[^-;a-z0-9\\s]', '', tolower(wos_data$DE))
wos_data$Keywords <- gsub('[^[[:space:]]\\-;a-z0-9]', '', tolower(wos_data$DE))
wos_data$Keywords <- trimws(gsub('[^[[:space:]]\\-;a-z0-9]', '', tolower(wos_data$DE)))
write.csv(wos_data, "../WebOfScience/Industry_Tagging/Outputs/wos_indtagged_final_wide_FEclean.csv", row.names=F)
#==============================================================================
# Script to take the results-sentiment classified abstracts and perform some
# initial analysis.
#==============================================================================
rm(list=ls())
library(ggplot2)
library(stargazer)
library(data.table)
library(dplyr)
# Read in data
setwd("/Volumes/GoogleDrive/.shortcut-targets-by-id/0B0efpUDNVRiVME9hOFNHaU1xVW8/Nutrition_BiasInScience/Regression_Analysis")
full_predictions <- read.csv("../Machine Learning/RL_Trained/Predictions/full_predictions_AdamW_grains.csv")
pred_dt <- setDT(full_predictions)
pred_dt <- na.omit(pred_dt) # Remove faulty na data
pred_dt <- pred_dt[,c("Abstract.Code", "Prob_NotPositive", "Prob_Positive", "Prediction")]
wos_indtagged <- read.csv("../WebOfScience/Industry_Tagging/Outputs/wos_indtagged_final_wide_FEclean.csv")
wos_indtagged <- setDT(wos_indtagged)
# Summary statistics data
summ_stats <- setDT(read.csv("../WebOfScience/Industry_Tagging/Outputs/Food Category Summary Statistics_v5.csv"))
summ_stats_nomiss <- rbind(summ_stats[!(Food.Code %in% c(134,166))],summ_stats[!(Food.Code %in% c(134,166))])
summ_stats_withmiss <- rbind(summ_stats, summ_stats)
# Create industry label variables
wos_indtagged[Is_Industry==0, Industry_Labels := "Not Industry Funded"]
wos_indtagged[Is_Industry==1, Industry_Labels := "Industry Funded"]
wos_indtagged[FU=="", Industry_Labels := "Missing Funding Data"]
# Top 100 industry labels
wos_indtagged[Is_Industry_Top100==0, Top100_Industry_Labels := "Not Top100 Industry Funded"]
wos_indtagged[Is_Industry_Top100==1, Top100_Industry_Labels := "Top100 Industry Funded"]
wos_indtagged[FU=="", Top100_Industry_Labels := "Missing Funding Data"]
industry_for_reg <- wos_indtagged[,c("Abstract.Code","Food.Code", "Food.Name","Is_Industry", "Is_Industry_Top100",
"Industry_Labels", "Top100_Industry_Labels", "SO"), with=FALSE]
industry_for_reg <- na.omit(industry_for_reg) # Remove faulty na data
colnames(industry_for_reg)[8] <- "Publication_Name"
# Create measure of "industry concentration" = # top 100 companies/# total companies
industry_counts <- industry_for_reg[,.(Tot_Industry = sum((Is_Industry)), Tot_Top100 = sum((Is_Industry_Top100))), by = Food.Code]
industry_counts$Industry_Concentration <- ifelse(industry_counts$Tot_Industry==0, 0, industry_counts$Tot_Top100/industry_counts$Tot_Industry)
all_industry_concentration <- sum(industry_counts$Tot_Top100)/sum(industry_counts$Tot_Industry)
all_industry_concentration # Full top100 concentration
# Get final data for regression
full_reg_df <- merge(industry_for_reg, pred_dt, by="Abstract.Code") # Output should be same length as pred_dt
full_reg_df$Prediction <- as.factor(full_reg_df$Prediction)
full_reg_df$Food.Code <- as.factor(full_reg_df$Food.Code)
full_reg_df$Is_Industry <- as.factor(full_reg_df$Is_Industry)
full_reg_df$Is_Industry_Top100 <- as.factor(full_reg_df$Is_Industry_Top100)
industry_sentiment_summary <- full_reg_df[,.(Counts=log(.N)),by=list(Industry_Labels, Prediction)]
industry_sentiment_top100_summary <- full_reg_df[,.(Counts=log(.N)),by=list(Top100_Industry_Labels, Prediction)]
# Save a separate regression data file that includes the article title + first 20 words
# The raw text is included so it can be matched on the raw data files in the future
industry_with_text <- wos_indtagged[,c("Abstract.Code","Food.Code", "AB", "TI","Food.Name","Is_Industry", "Is_Industry_Top100",
"Industry_Labels", "Top100_Industry_Labels", "SO", "Pub_Year", "Keywords"), with=FALSE]
colnames(industry_with_text)[c(4, 10)] <- c("Title", "Journal")
industry_with_text$AB <- substr(industry_with_text$AB, 1,200) # First 200 characters of AB
reg_data <- merge(industry_with_text, pred_dt, by="Abstract.Code", all.y = TRUE)
write.csv(reg_data, "regression_data.csv", row.names=FALSE)
#==============================================================================
# Main regression analysis with rolling fixed effect controls.
# Missing funding assumed to be non-industry.
#==============================================================================
rm(list=ls())
library(ggplot2)
library(stargazer)
library(data.table)
library(dplyr)
setwd("/Volumes/GoogleDrive/.shortcut-targets-by-id/0B0efpUDNVRiVME9hOFNHaU1xVW8/Nutrition_BiasInScience/Regression_Analysis")
reg_data <- setDT(read.csv('regression_data.csv'))
View(reg_data)
top25_ind_kw <- setDT(read.csv("Distribution_Analysis/subject_ind.csv"))
View(reg_data)
# Create factors for missing covariates
reg_data[Pub_Year < 1000, Pub_Year := 0] # 0 will be missing year level
reg_data[is.na(Journal) | Journal == "", Journal := "Missing"] # "Missing" will be missing journal level
nrow(reg_data[Keywords == ""])
nrow(reg_data[is.na(Keywords)])
reg_data$Missing_Keywords <- reg_data$Keywords == "" # Indicator for missing keywords
reg_data$Food.Code <- as.factor(reg_data$Food.Code)
reg_data$Pub_Year <- as.factor(reg_data$Pub_Year)
nrow(reg_data[Pub_Year == 0])
nrow(reg_data[Journal == "Missing"])
# 1) Main industry effect
logit_1 <- glm(Prediction ~ Is_Industry, family=binomial(link="logit"), data=reg_data)
summary(logit_1)
summary(logit_2)
# 2) Food Group FE
logit_2 <- glm(Prediction ~ Is_Industry + Food.Code, family=binomial(link="logit"), data=reg_data)
summary(logit_2)
# 3) Publication Year FE
logit_3 <- glm(Prediction ~ Is_Industry + Food.Code + Pub_Year, family=binomial(link="logit"), data=reg_data)
summary(logit_3)
logit_2 <- glm(Prediction ~ 0 + Is_Industry + Food.Code, family=binomial(link="logit"), data=reg_data)
summary(logit_2)
# 3) Publication Year FE
logit_3 <- glm(Prediction ~ 0 + Is_Industry + Food.Code + Pub_Year, family=binomial(link="logit"), data=reg_data)
summary(logit_3)
# 4) Publication Journal FE
logit_4 <- glm(Prediction ~ 0 + Is_Industry + Food.Code + Pub_Year + Journal, family=binomial(link="logit"), data=reg_data)
summary(logit_4)
# 5) Top 25 Keywords FE
# Need to create indicator for whether one of the keywords for the paper is one of the top 25
reg_data_long <- reg_data %>% separate_rows(Keywords, sep=';')
View(reg_data_long)
rm(list=ls())
library(ggplot2)
library(stargazer)
library(data.table)
library(dplyr)
setwd("/Volumes/GoogleDrive/.shortcut-targets-by-id/0B0efpUDNVRiVME9hOFNHaU1xVW8/Nutrition_BiasInScience/Regression_Analysis")
jcr_dt <- setDT(read.csv("jcr_impact_factors.csv"))
wos_indtagged <- setDT(read.csv("../WebOfScience/Industry_Tagging/Outputs/wos_indtagged_final_wide_FEclean.csv"))
grains_journals <- unique(wos_indtagged[Food.Code <= 22,SO])
grains_journals_jcr <- jcr_dt[Journal %in% grains_journals,]
nrow(grains_journals_jcr[is.na(Impact_Factor)])
View(top25_ind_kw)
View(grains_journals_jcr)
length(grains_journals)
View(jcr_dt)
grains_journals
missing <- grains_journals[!(grains_journals %in% jcr_dt$Journal)]
length(missing)
missing
nrow(grains_journals_jcr[is.na(Impact_Factor)])
#==============================================================================
# Main regression analysis with rolling fixed effect controls.
# Missing funding assumed to be non-industry.
#==============================================================================
rm(list=ls())
library(ggplot2)
library(stargazer)
library(data.table)
library(tidyr)
library(stringr)
library(dplyr)
setwd("/Volumes/GoogleDrive/.shortcut-targets-by-id/0B0efpUDNVRiVME9hOFNHaU1xVW8/Nutrition_BiasInScience/Regression_Analysis")
reg_data <- setDT(read.csv('regression_data.csv'))
top25_ind_kw <- setDT(read.csv("Distribution_Analysis/subject_ind.csv"))
# Create factors for missing covariates
reg_data[Pub_Year < 1000, Pub_Year := 0] # 0 will be missing year level
reg_data[is.na(Journal) | Journal == "", Journal := "Missing"] # "Missing" will be missing journal level
reg_data$Missing_Keywords <- reg_data$Keywords == "" # Indicator for missing keywords
reg_data$Food.Code <- as.factor(reg_data$Food.Code)
reg_data$Pub_Year <- as.factor(reg_data$Pub_Year)
reg_data_long <- reg_data %>% separate_rows(Keywords, sep=';')
reg_data_long$Keywords <- trimws(reg_data_long$Keywords)
View(top25_ind_kw)
sum(!(top25_ind_kw$Subject %in% reg_data_long$Keywords))
reg_data_long$Top_KW <- reg_data_long$Keywords %in% top25_ind_kw$Subject
reg_data_kw <- setDT(reg_data_long)[,.(Top_KW = max(Top_KW)), by=setdiff(names(reg_data_long), c("Keywords", "Top_KW"))]
View(reg_data_kw)
reg_data_kw <- setDT(reg_data_long)[,.(Top_KW = max(Top_KW), N_Top_KW=sum(Top_KW)), by=setdiff(names(reg_data_long), c("Keywords", "Top_KW"))]
View(reg_data_kw)
#==============================================================================
# Author: Richard Liu
# Description: Script to compute and visualize the distribution of publication dates between
# missing/non-missing funding sources and between industry/non-industry sources
#==============================================================================
rm(list = ls())
library(dplyr)
library(tidyr)
library(data.table)
library(stringr)
library(gridExtra)
library(ggplot2)
library(chisq.posthoc.test)
setwd("/Volumes/GoogleDrive/.shortcut-targets-by-id/0B0efpUDNVRiVME9hOFNHaU1xVW8/Nutrition_BiasInScience/Regression_Analysis")
wos_data <- setDT(read.csv("../WebOfScience/Industry_Tagging/Outputs/wos_indtagged_final_wide_v1.csv"))
wos_data$Funding_Missing <- (wos_data$FU_stripped_lower == ' missing ')
wos_data$Pub_Year <- str_extract(wos_data$PY, "\\d+") # Extract numeric values from publication year column
wos_data$PD <- str_extract(wos_data$PD, "\\d+")
wos_data$EA <- str_extract(wos_data$EA, "\\d+")
wos_data$Pub_Year <- as.numeric(wos_data$Pub_Year)
wos_data$PD <- as.numeric(wos_data$PD)
wos_data$EA <- as.numeric(wos_data$EA)
wos_data[is.na(PD), PD := 0]
wos_data[is.na(EA), EA := 0]
wos_data[is.na(Pub_Year) | (Pub_Year > 0 & Pub_Year < 1000), Pub_Year := pmax(PD, EA, na.rm = TRUE)]
# Group together the different editions/volumes of the same journal
wos_data$Journal <- gsub(", VOL[^,]*", "", wos_data$SO)
wos_data$Journal <- gsub(",[^,]+EDITION", "", wos_data$Journal)
View(wos_data)
cleaned_journals <- unique(wos_data$Journal)
grains_journals <- unique(wos_data[Food.Code <= 22, SO])
grains_journals <- as.character(unique(wos_data[Food.Code <= 22, SO]))
test <- grains_journals[!(grains_journals %in% cleaned_journals)]
test
wos_data$Journal <- trimws(wos_data$Journal)
# Save version of wos file with cleaned publiation year and keywords
wos_data[is.na(DE) | DE == "", DE := ID]
wos_data$Keywords <- trimws(gsub('[^[[:space:]]\\-;a-z0-9]', '', tolower(wos_data$DE)))
write.csv(wos_data, "../WebOfScience/Industry_Tagging/Outputs/wos_indtagged_final_wide_FEclean.csv", row.names=F)
#==============================================================================
# Script to take the results-sentiment classified abstracts and perform some
# initial analysis.
#==============================================================================
rm(list=ls())
library(ggplot2)
library(stargazer)
library(data.table)
library(dplyr)
# Read in data
setwd("/Volumes/GoogleDrive/.shortcut-targets-by-id/0B0efpUDNVRiVME9hOFNHaU1xVW8/Nutrition_BiasInScience/Regression_Analysis")
full_predictions <- read.csv("../Machine Learning/RL_Trained/Predictions/full_predictions_AdamW_grains.csv")
pred_dt <- setDT(full_predictions)
pred_dt <- na.omit(pred_dt) # Remove faulty na data
pred_dt <- pred_dt[,c("Abstract.Code", "Prob_NotPositive", "Prob_Positive", "Prediction")]
wos_indtagged <- read.csv("../WebOfScience/Industry_Tagging/Outputs/wos_indtagged_final_wide_FEclean.csv")
wos_indtagged <- setDT(wos_indtagged)
# Summary statistics data
summ_stats <- setDT(read.csv("../WebOfScience/Industry_Tagging/Outputs/Food Category Summary Statistics_v5.csv"))
summ_stats_nomiss <- rbind(summ_stats[!(Food.Code %in% c(134,166))],summ_stats[!(Food.Code %in% c(134,166))])
summ_stats_withmiss <- rbind(summ_stats, summ_stats)
# Create industry label variables
wos_indtagged[Is_Industry==0, Industry_Labels := "Not Industry Funded"]
wos_indtagged[Is_Industry==1, Industry_Labels := "Industry Funded"]
wos_indtagged[FU=="", Industry_Labels := "Missing Funding Data"]
# Top 100 industry labels
wos_indtagged[Is_Industry_Top100==0, Top100_Industry_Labels := "Not Top100 Industry Funded"]
wos_indtagged[Is_Industry_Top100==1, Top100_Industry_Labels := "Top100 Industry Funded"]
wos_indtagged[FU=="", Top100_Industry_Labels := "Missing Funding Data"]
industry_for_reg <- wos_indtagged[,c("Abstract.Code","Food.Code", "Food.Name","Is_Industry", "Is_Industry_Top100",
"Industry_Labels", "Top100_Industry_Labels", "SO"), with=FALSE]
industry_for_reg <- na.omit(industry_for_reg) # Remove faulty na data
colnames(industry_for_reg)[8] <- "Publication_Name"
# Create measure of "industry concentration" = # top 100 companies/# total companies
industry_counts <- industry_for_reg[,.(Tot_Industry = sum((Is_Industry)), Tot_Top100 = sum((Is_Industry_Top100))), by = Food.Code]
industry_counts$Industry_Concentration <- ifelse(industry_counts$Tot_Industry==0, 0, industry_counts$Tot_Top100/industry_counts$Tot_Industry)
all_industry_concentration <- sum(industry_counts$Tot_Top100)/sum(industry_counts$Tot_Industry)
all_industry_concentration # Full top100 concentration
# Get final data for regression
full_reg_df <- merge(industry_for_reg, pred_dt, by="Abstract.Code") # Output should be same length as pred_dt
full_reg_df$Prediction <- as.factor(full_reg_df$Prediction)
full_reg_df$Food.Code <- as.factor(full_reg_df$Food.Code)
full_reg_df$Is_Industry <- as.factor(full_reg_df$Is_Industry)
full_reg_df$Is_Industry_Top100 <- as.factor(full_reg_df$Is_Industry_Top100)
industry_sentiment_summary <- full_reg_df[,.(Counts=log(.N)),by=list(Industry_Labels, Prediction)]
industry_sentiment_top100_summary <- full_reg_df[,.(Counts=log(.N)),by=list(Top100_Industry_Labels, Prediction)]
# Save a separate regression data file that includes the article title + first 20 words
# The raw text is included so it can be matched on the raw data files in the future
industry_with_text <- wos_indtagged[,c("Abstract.Code","Food.Code", "AB", "TI","Food.Name","Is_Industry", "Is_Industry_Top100",
"Industry_Labels", "Top100_Industry_Labels", "Journal", "Pub_Year", "Keywords"), with=FALSE]
colnames(industry_with_text)[4] <- c("Title")
industry_with_text$AB <- substr(industry_with_text$AB, 1,200) # First 200 characters of AB
reg_data <- merge(industry_with_text, pred_dt, by="Abstract.Code", all.y = TRUE)
write.csv(reg_data, "regression_data.csv", row.names=FALSE)
#==============================================================================
# Main regression analysis with rolling fixed effect controls.
# Missing funding assumed to be non-industry.
#==============================================================================
rm(list=ls())
library(ggplot2)
library(stargazer)
library(data.table)
library(tidyr)
library(stringr)
library(dplyr)
setwd("/Volumes/GoogleDrive/.shortcut-targets-by-id/0B0efpUDNVRiVME9hOFNHaU1xVW8/Nutrition_BiasInScience/Regression_Analysis")
reg_data <- setDT(read.csv('regression_data.csv'))
top25_ind_kw <- setDT(read.csv("Distribution_Analysis/subject_ind.csv"))
# Create factors for missing covariates
reg_data[Pub_Year < 1000, Pub_Year := 0] # 0 will be missing year level
reg_data[is.na(Journal) | Journal == "", Journal := "Missing"] # "Missing" will be missing journal level
reg_data$Missing_Keywords <- reg_data$Keywords == "" # Indicator for missing keywords
reg_data$Food.Code <- as.factor(reg_data$Food.Code)
reg_data$Pub_Year <- as.factor(reg_data$Pub_Year)
#' ---
#' author: "Richard Liu"
#' output: pdf
#' description: Comparing scraped journal impact data and grains coverage
#' ---
rm(list=ls())
library(dplyr)
library(tidyr)
library(data.table)
library(stringr)
library(gridExtra)
library(readxl)
setwd("/Volumes/GoogleDrive/.shortcut-targets-by-id/0B0efpUDNVRiVME9hOFNHaU1xVW8/Nutrition_BiasInScience/Regression_Analysis/Journal_Analysis")
jcr <- setDT(read.csv("jcr_impact_factors.csv"))
sjr <- setDT(read.csv("sjr_scrape.csv"))
View(jcr)
View(sjr)
# Check coverage with each other
full_data <- merge(jcr, sjr, by='Journal')
View(full_data)
full_data$X <- NULL
colnames(full_data)[c(3,7)] <- c("Impact_Year", "SJR_Year")
colnames(full_data)[c(3,6)] <- c("Impact_Year", "SJR_Year")
nrow(full_data[Impact_Factor == ""])
full_data[Impact_Factor == "", Impact_Factor := NaN]
View(full_data)
jcr_no_sjr <- full_data[is.na(SJR_Score) & !is.na(Impact_Factor)]
View(jcr_no_sjr)
sjr_no_jcr <- full_data[is.na(Impact_Factor) & !is.na(SJR_Score)]
sjr_no_jcr_h <- full_data[is.na(Impact_Factor) & !is.na(H_Index)]
View(sjr_no_jcr_h)
View(sjr_no_jcr)
# Check grains coverage
wos <- setDT(read.csv("../../WebOfScience/Industry_Tagging/Outputs/wos_indtagged_final_wide_FEclean.csv"))
grains_journals <- wos[Food.Code <= 22, Journal]
grains_journals <- unique(wos[Food.Code <= 22, Journal])
full_data$Grains_Journal <- full_data$Journal %in% grains_journals
View(full_data)
grains_missing <- full_data[Grains_Journal & (is.na(Impact_Factor) | is.na(SJR_Score))]
View(grains_missing)
nrow(grains_missing[is.na(SJR_Score)])
grains_missing_sjr <- grains_missing[is.na(SJR_Score)]
View(grains_missing_sjr)
length(grains_journals)
write.csv(full_data, "full_journals_data.csv", row.names=F)
grains_missing_jcr <- grains_missing[is.na(Impact_Factor)] #182/418
#==============================================================================
# Main regression analysis with rolling fixed effect controls.
# Missing funding assumed to be non-industry.
#==============================================================================
rm(list=ls())
library(ggplot2)
library(stargazer)
library(data.table)
library(tidyr)
library(stringr)
library(dplyr)
setwd("/Volumes/GoogleDrive/.shortcut-targets-by-id/0B0efpUDNVRiVME9hOFNHaU1xVW8/Nutrition_BiasInScience/Regression_Analysis")
reg_data <- setDT(read.csv('regression_data.csv'))
top25_ind_kw <- setDT(read.csv("Distribution_Analysis/subject_ind.csv"))
# Create factors for missing covariates
reg_data[Pub_Year < 1000, Pub_Year := 0] # 0 will be missing year level
reg_data[is.na(Journal) | Journal == "", Journal := "Missing"] # "Missing" will be missing journal level
reg_data$Missing_Keywords <- reg_data$Keywords == "" # Indicator for missing keywords
reg_data$Food.Code <- as.factor(reg_data$Food.Code)
reg_data$Pub_Year <- as.factor(reg_data$Pub_Year)
# ============= All Food Groups Regression w/ Fixed Effects ===========
# 1) Main industry effect
logit_1 <- glm(Prediction ~ Is_Industry, family=binomial(link="logit"), data=reg_data)
summary(logit_1)
# 2) Food Group FE
logit_2 <- glm(Prediction ~ 0 + Is_Industry + Food.Code, family=binomial(link="logit"), data=reg_data)
summary(logit_2)
# 3) Publication Year FE
logit_3 <- glm(Prediction ~ Is_Industry + Food.Code + Pub_Year, family=binomial(link="logit"), data=reg_data)
# 4) Publication Journal FE
logit_4 <- glm(Prediction ~ Is_Industry + Food.Code + Pub_Year + Journal, family=binomial(link="logit"), data=reg_data)
# 5) Top 25 Keywords FE
# Need to create indicator for whether one of the keywords for the paper is one of the top 25
reg_data_long <- reg_data %>% separate_rows(Keywords, sep=';')
reg_data_long$Keywords <- trimws(reg_data_long$Keywords)
reg_data_long$Top_KW <- reg_data_long$Keywords %in% top25_ind_kw$Subject
reg_data_kw <- setDT(reg_data_long)[,.(Top_KW = max(Top_KW), N_Top_KW=sum(Top_KW)), by=setdiff(names(reg_data_long), c("Keywords", "Top_KW"))]
View(reg_data_kw)
# Logit
logit_5 <- glm(Prediction ~ Is_Industry + Food.Code + Pub_Year + Journal + Top_KW + Missing_Keywords,
family=binomial(link="logit"), data=reg_data_kw)
# 6) # Top Keywords
logit_6 <- glm(Prediction ~ Is_Industry + Food.Code + Pub_Year + Journal + N_Top_KW + Missing_Keywords,
family=binomial(link="logit"), data=reg_data_kw)
install.packages("rowr")
